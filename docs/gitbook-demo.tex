% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Workflows for processing microbial amplicon sequences},
  pdfauthor={Christian Krohn, PhD, RMIT University},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Workflows for processing microbial amplicon sequences}
\author{\href{https://www.rmit.edu.au/contact/staff-contacts/academic-staff/k/krohn---christian}{Christian Krohn, PhD, RMIT University}}
\date{2022-07-08}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{about-this-gitbook}{%
\chapter{About this GitBook}\label{about-this-gitbook}}

\includegraphics{./img/data.jpeg}

This GitBook contains workflows for students who want to get started with sequencing microbial amplicons on a Miseq instrument and then process and analyse ASV-based data. It is a compilation of workflows that I have gotten accustomed to during my PhD at La Trobe University with help of many amazing students and colleagues over the years. But it is by no means comprehensive.

One of the biggest hurdles for students that embark on sequencing environmental DNA for the first time, is the effort that is required to understand the various coding languages, file types and formats, packages or platforms that are involved (think Unix, Linux, Slurm, Qiime, R, RMarkdown, python, conda, ggplot, docker, GitHub, data instances\ldots) before they even can start looking at exploring the data for biological meaning and producing publishable output.

For example, this GitBook is made in \href{https://rstudio.com}{Rstudio}, using the \href{https://rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf}{\texttt{rmarkdown} markup language}, rendered using the \href{https://bookdown.org/yihui/bookdown/get-started.html}{\texttt{bookdown} package}, and hosted on \href{https://github.com/}{GitHub}. It took me at least five years before I was ready to create and host this content for students. And even then I needed this awesome \href{https://cjvanlissa.github.io/gitbook-demo/index.html}{Bookdown template} from Dr.~Caspar van Lissa to get started with Bookdown. So it is really helpful to have something to start with.

It was not easy to get over the initial hurdles for me too, so I thought I'd try to compile my workflows. This may help students to get a head start but also helps to scrutinise my approach in the hope that others can point me to better ways of doing things.

\textbf{Browse to your level}

Everyone is at a different level in the journey to exploring microbial diversity. Some have absolutely no idea where to start and others have created their first phylogentic trees with ggtree or similar. The most important prerequisites are patience and persistence\ldots{} :). Just explore the different chapters and see where it takes you. In fact you will be amazed how much you can achieve with mininal coding knowledge.

\textbf{Get in touch}

We work at the Andy Ball lab, RMIT University, Bundoora, Melbourne. Email me or comment on the discussion section of the \href{https://github.com/chrismitbiz/ABlab-workflows/discussions/}{GitHub repository} for this GitBook. You will need to get a GitHub account to join the discussion. Its free.

\hypertarget{gettingstarted}{%
\chapter{Getting started}\label{gettingstarted}}

Prerequisites will be listed in each sub chapter separately.
Following a few general comments:

\textbf{Access to computational resources}

To process data you will need a personal computer or online computational resources with sufficient number of CPUs, working memory and storage. If you want to run most of the processing on a personal computer then I'd recommend a minimum of \textbf{4 CPUs, 16 Gb RAM and 500 Gb storage}. However, please note that some alignment tasks require more RAM, hence some of the steps (e.g.~taxonomic classifications) may require a more powerful resource. The advantage of using a personal computer is that you can frequently update the latest packages without relying on a university administrator to do that for you. Furthermore, with a personal computer you wont need to share with others, making it easier to run the packages without a workload manager.

Some university may provide their own access to a high performance computer (HPC) for staff and students and likely have certain packages pre-installed. In such cases a batch workload manger such as Slurm may be used, to manage multiple users trying to run computationally heavy jobs. This works fine too but its a bit of a hassle and I would use such an HPC only if really necessary, i.e.~smaller jobs on my personal computer and bigger jobs on the HPC.

I am not aware that RMIT University offers that type of service yet.

In Australia there is also the \href{https://ardc.edu.au/services/nectar-research-cloud/}{ARDC Nectar Research Cloud}, which provides free resources to Universities. This is basically like providing access to an online Linux computer with complete freedom to install any packages as if it was your own. If you are a student you can \href{https://dashboard.rc.nectar.org.au/}{create a trial account} with your student email address (Using your ID only; without the student.edu.au) and check out how it all works, albeit with limited resources. The Nectar service also provides \href{https://tutorials.rc.nectar.org.au/}{tuturials for starters}. However, to get serious with Nectar, you have to request more resources, which is a fairly straight-forward process. Check out the \href{https://support.ehelp.edu.au/support/solutions/articles/6000068044-managing-an-allocation}{eligibilities} and chat to your supervisor if you are interested.

There are also commercial options, which probably offer very similar type of services to Nectar.

In addition, there is the awesome \href{https://usegalaxy.org.au/}{Galaxy Australia platform}, which is an open web-based resource that also contains many tutorials and workflows specific to bioinformatics. Definitely worth checking that out too.

This workbook will be based on using the Nectar Research cloud.

\textbf{File storage}

Working with sequencing data may require you do handle large amounts of data. From experience, a run of a 600µl pool (6-10pM) of amplicons in 2x301 Miseq cycles may produce around 10-20 Gb of FASTQ files. \href{https://sapac.support.illumina.com/bulletins/2018/01/approximate-sizes-of-sequencing-run-output-folders.html}{Much more} can be expected for NextSeq runs or for long-read sequencing.

Once a Miseq run has finished it is typically available to download from the \href{https://basespace.illumina.com/dashboard}{Illumina Basespace} account. After a few runs, the data can accumulate quickly. Hence, \textbf{persistent storage space} is required. As long as you have the Basespace account you can keep a copy of the FASTQ file online. However, Basespace may delete your data if the account has been inactive for more than 6 months. Alternatively, data can be upload onto a public repository for persistent storage, such as the \href{https://www.ncbi.nlm.nih.gov/sra}{Sequence Reads Archives}. If you have access to a research cloud or commercial computational resources then you may store data on those. Personally, I prefer a 1TB solid state harddrive.

\textbf{Command Line}

Many of the tools used are managed through command line. Cloud servers are also accessed through command line. That means there is no graphical user interphase and all commands, including installation and running of packages are done through lines of codes on a simple window interphase. This window is called Terminal (Mac, Linux) or Command Prompt application (Windows). There are lots of online sources to help you get started with command line. One example here: \url{https://towardsdatascience.com/a-quick-guide-to-using-command-line-terminal-96815b97b955}

\textbf{Data storage browser}

To upload, download, move, rename files on any cloud computer or high performance computer (HPC) at your institution, you will also require a cloud storage browser, such as \href{https://docs.cyberduck.io/}{Cyberduck} (Mac) or \href{https://www.putty.org/}{PuTTY} (Windows).

\textbf{R and R studio}

R has become an essential research language. If you want to progress in research it is almost inevitable for you to learn. Just do it. :). Perhaps start here if you dont know where else to start: \url{https://education.rstudio.com/learn/beginner/}. It lists some great step-by-step tutorials on how to install R and Rstudio and then explains the basics.

If you are like me, then you simply copy and paste code from other sites and see what happens. This book should provide you with the required information to enable you to follow the workflows. In case you get stuck, you can either chat to me directly or write on the GitHub Discussion Page for this GitBook (\href{(https://github.com/chrismitbiz/ABlab-workflows/discussions/)}{chrismitbiz/ABlab-workflows/discussions/}).

At the same time you will want to do short courses (eg. from edX \href{https://www.edx.org/course/data-science-r-basics}{Data Science: R Basics}) that delve a little deeper into different data structures such as data frames, matrices, lists and the syntax to arrange stuff. Over time you understand what different lines of code mean and can trouble shoot when things don't work. And things often don't work. You will get used to troubleshooting code :).

\textbf{Environment managers}

Package and environment managers are extremely useful for your workflows. They help to install and run software packages such as qiime2, into individual `environments' independent of your operating system. The environment manager is installed once and from there you use it to install individual packages. The most commonly used environment manager is Conda. Learn more about it here: \url{https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html}

Conda can be installed by either installing Anaconda or Miniconda. Boths works the same way. Anaconda requires 5 Gb of discspace and installs everything you can possibly need, while Miniconda is just the raw bones and lets you install things one by one.

Most of the commands to manage environments are done in command line using a terminal. There is also a graphical installer that makes handling the environments a little more visual. That would be my go to. Learn more here: www.anaconda.com.

Once conda installed you can always check what environments are installed with the command \texttt{conda\ env\ list}. In my case the output looks like this:\\
\includegraphics{./img/condaenvs.png}\\

There are several other environment managers. We are also using \href{https://www.docker.com/}{Docker} in our work but the learning curve is a bit steeper so we wont get into it here.

\textbf{Explore the help functions}

Packages such as qiime2 have great resources to help you understand how to run any of the available commands. For example if you dont know what input parameters are available for the qiime command\\
\texttt{qiime\ feature-classifier\ classify-sklearn}, you can simply enter this command into your Terminal (with the qiime environment activated, if it is installed with Conda) and add a \texttt{-\/-help} at the end: \texttt{qiime\ feature-classifier\ classify-sklearn\ -\/-help}.

Or you just want to know what other commands qiime has in its repertoire, you can run \texttt{qiime\ -\/-help}.

For example, this

\begin{verbatim}
conda activate qiime2-2022.2  
qiime --help  
\end{verbatim}

Gives\\
\includegraphics{./img/qiimehelp.png}

\textbf{GitHub account}

This GitBook, including all its files, is hosted on one of my GitHub repositories (\url{https://github.com/chrismitbiz/ABlab-workflows}). If you have any comments you can ask a question on the Discussion Page of this repository. You require a GitHub account to do that. It is free.

\textbf{Workflows from other lab groups}

Searching for ``amplicon-sequencing'' under ``Topics'' on GitHub gave 37 results.

\begin{itemize}
\item
  nf-core/ampliseq\\
  On the top of that list is the \href{https://github.com/topics/amplicon-sequencing}{nf-core/ampliseq} pipeline developed by the nf-core community. It is based on a software called \href{https://www.nextflow.io/}{Nextflow} which allows to put different processes into a pipeline. This is great for doing things a little more reproducible but it requires you to be fairly knowledgeable with Linux, container software, config files etc.. Not great to learn stuff for beginners.
\item
  Tools-Microbiome-Analysis\\
  \href{https://microsud.github.io/Tools-Microbiome-Analysis/}{Tools-Microbiome-Analysis} is a websites containing a comprehensive list of R packages and, more importantly, tutorials related to analysis of microbial amplicons and ecology. Really good reference to go back to every now and then. \textbf{Highly recommended}.
\item
  grimmlab/MicrobiomeBestPracticeReview\\
  Essential paper to read (\href{https://academic.oup.com/bib/article/22/1/178/5678919}{Current challenges and best-practice protocols for microbiome analysis, 2021}) and a great \href{https://github.com/grimmlab/MicrobiomeBestPracticeReview}{workflow resource} on GitHub. \textbf{Essential read}. The Grimm lab in Munich published this paper as well as developed a python and R-based workflow that assists with the recommended best practices (amplicon as well as metagenomic workflows).
\item
  KasperSkytte/ampvis2\\
  \href{https://kasperskytte.github.io/ampvis2/articles/ampvis2.html}{Ampviz2} is an R-package to visualise and analyse 16S rRNA amplicon data. It is always more convenient to have packages that have the details and optics worked out for you. Like phyloseq, the ampviz2 package combines different tables from data (E.g. otu table, taxonomic table, phylogenetic tree, sample data etc) and then provides different functions to apply to that combined object to visualise the data.
\end{itemize}

\hypertarget{miseq-library-preps}{%
\chapter{Miseq library preps}\label{miseq-library-preps}}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\textbf{Some info on getting your run ready}

In our lab we usually prepare a 6-10 pM pool of libraries following the \href{https://sapac.support.illumina.com/content/dam/illumina-support/documents/documentation/chemistry_documentation/16s/16s-metagenomic-library-prep-guide-15044223-b.pdf}{16S Metagenomic Sequencing Library
Preparation} protocol from Illumina with 10\% Phix. \textbf{Download it, study it and refer to it throughout your labwork}. This protocol is not limited to 16S amplicons. You can use it for fungal (e.g.~ITS) amplicons too. In the last section there are also valuable tips for pre‐PCR and post‐PCR lab procedures. There is also some additional Illumina guidance here: \url{https://sapac.support.illumina.com/downloads/16s_metagenomic_sequencing_library_preparation.html}

\hypertarget{overview}{%
\subsection{Overview}\label{overview}}

The protocol encompasses the following steps (copied out of the protocol):
\includegraphics{./img/16Slibraryprotocolworkflow.png}

Please note that only \textbf{one amplicon target} should be sequenced in each run. That means it is not recommended to sequence for example two different 16S amplicons or one 16S amplicon plus an ITS amplicon. Sequencing different amplicon lengths will result in reduced quality as the the Miseq flowcell will preferentially cluster shorter fragments. However, it is not impossible. We have tried to run four different amplicons on one flowcell before, although with mixed results.

Labwork may take four weeks from DNA extraction to Miseq loading. Double that if you have not done it before. You probably realised that there is quite a bit of labwork required before you finally get FASTQ files and subsequent ASV/taxonomy tables to analyse - the ultimate goal here. The more samples are planned to be included into the pool, the more work and consumables are needed. Illumina recommends no more than 96 samples on one flowcell to get enough depths (i.e.~\textgreater{} 100,000 reads per sample). But we have sequenced around 196 samples on one flowcell before, which captured sufficient reads and diversity. This depends on how much detail and rare taxa you want to capture. The less samples you include, the more data and depth for each sample.

Also note that you will require over 20 boxes of tips (mostly 200 µl) for each 96-well plate, so plan your consumables ahead of time. Most of the tips will be used for the PCR clean ups.

After all steps in the above protocol are completed, the Miseq will be loaded with a 600 µl pool, followed by 56 hours of run-time for 2x301 cycles before FASTQ files are available for download from BaseSpace.

\includegraphics{./img/16s-library-prep-overview.png}

Throughout the 56 hours run you can check how things are tracking either on the BaseSpace website or the BaseSpace phone app. Understanding different QC metrics will help identifying potential problems with the pool or the instrument. It is important to note that this is a bit of a nerve-wrecking moment in the process, as it remains unclear until the metrics come through after around the 21st cycle, whether or not the run will be successful.\\
\strut \\

\hypertarget{workflow}{%
\section{Workflow}\label{workflow}}

\hypertarget{dna-extraction}{%
\subsection{DNA extraction}\label{dna-extraction}}

\begin{figure}
\centering
\includegraphics{./img/dna.jpeg}
\caption{``DNA extracts''}
\end{figure}

We commonly extract DNA from soils or from wastewater sludges using the \emph{DNeasy Powersoil Pro Kit} (Qiagen) for both, soil and sludge. It results in high quality DNA and includes a bead beating step to make sure the gram positive cells are sufficiently broken up. Although, I don't have evidence for that. In fact, please let us know on the \href{https://github.com/chrismitbiz/ABlab-workflows/discussions}{GitHub Discussion Page} for this GitBook if you have any comments on that as we are always looking for ways to improve things.

Soil

\begin{itemize}
\tightlist
\item
  For soils, the protocol of the extraction kit is followed, weighing in 0.25 g of soil and making sure to record the exact weights used.\\
\item
  Also make sure to measure the water content of the soil to be able to report DNA concentrations per dry weight of soil.
\end{itemize}

Sludge

\begin{itemize}
\tightlist
\item
  For DNA extraction of sludge, 0.5 ml is added into an empty 1.7 ml extraction tube (without extraction beads), then centrifuged at 5000 rpm for 5 minutes before removing the supernatent.
\item
  Extraction beads are then added and the Powersoil protocol followed.
\item
  Measure Total Solids (TS) of the sludge to report DNA concentrations per gram of TS.\\
  \strut \\
\end{itemize}

\hypertarget{dna-quality-and-concentration}{%
\subsection{DNA quality and concentration}\label{dna-quality-and-concentration}}

Once you have extracted DNA it is necessary to confirm its quality and concentration.

\begin{itemize}
\tightlist
\item
  Quality is assessed using a Nanodrop Spectrophotometer. You are aiming for \textbf{260/280 Ratio of 1.8 and a 260/230 Ratio of 2-2.2}. This is explained in the \href{https://dna.uga.edu/wp-content/uploads/sites/51/2019/02/Note-on-the-260_280-and-260_230-Ratios.pdf}{T042‐TECHNICAL BULLETIN}. The Nanopore also estimates DNA concentrations but it is not recommended to use these concentrations measurements to normalise DNA. It is more accurate and consistent with a Qubit Fluorometer.\\
\item
  DNA concentrations are best measured using fluorescent dye-based methods such as the \textbf{Qubit Fluorometer}. It is easist to use the `\emph{1X}' assays such as the \emph{Qubit 1X dsDNA BR Assay Kit} or the \emph{Qubit 1X dsDNA HS Assay Kit}

  \begin{itemize}
  \tightlist
  \item
    Broad range (BR) Kit for quantifying extracted environmental DNA and afte the 2nd stage PCR.
  \item
    High Sensitivity (HS) Kit for quantifying the pool at the end.
  \end{itemize}
\end{itemize}

\begin{figure}
\centering
\includegraphics{./img/qubit.png}
\caption{``The ready-mix working solution for Qubit, which includes dye and buffer. No preparation/premixing required.''}
\end{figure}

\hypertarget{dna-normalisation}{%
\subsection{DNA normalisation}\label{dna-normalisation}}

The next step is to normalise all DNA extracts to 5 ng/µl.

\begin{itemize}
\tightlist
\item
  Dilute DNA to equal concentrations, either in a fresh batch of PCR-grade 10mM Tris buffer (pH 8.5) or in PCR-grade water\\
\item
  Set a pre-define DNA volume, say 2µl for all extracts, and add Tris or water as calculated with\\
  \[ C_1V_1 = V_2 C_2 \]\\
  \strut \\
\end{itemize}

\hypertarget{first-and-second-pcr---amplicon-pcr-and-indexing-pcr}{%
\subsection{First and second PCR - Amplicon PCR and Indexing PCR}\label{first-and-second-pcr---amplicon-pcr-and-indexing-pcr}}

Follow the \href{https://sapac.support.illumina.com/content/dam/illumina-support/documents/documentation/chemistry_documentation/16s/16s-metagenomic-library-prep-guide-15044223-b.pdf}{16S Metagenomic Sequencing Library
Preparation} Guide. Include a negative control and sequence that as well. This allows you to assess background and cross contamination. The whole workflow is best done in 96-well plates, using multi-channel pipettes to avoid indexing mistakes across wells.

Sample list and plate layout

Ensure that all sample IDs, plate-layout and each unique Illumina index carefully planned and printed out before you start your PCRs. Perhaps, stick the layout onto the Bio-safety cabinet where you do most of the pipetting and keep checking you are on the right well and sample. Label all plates even if you discard them afterwards. Labwork can be stressful and you need to trace all your steps across each well of the plate because otherwise you risk that you have indexed the wrong samples and it will become impossible to know which sample you have sequenced.

Primers

Any primers you order for the Miseq will have to include the \textbf{Illumina overhang adapter}. The sequence of the overhang adapter is added \textbf{in front} of the gene-specific primer sequences. There are two overhang adapters; one for the forward primers and one for the reverse primer. Check out the relevant information on primers from page 3 the 16S Metagenomic Sequencing Library Preparation Guide.

Clean-ups

It is worth doing the clean-ups in a separate deep-well plate (``MIDI'' plate, see consumables below) to avoid any blow-out of dried beads at the end of the clean-up. This process is explained in the above mentioned guide too. Tip: Handle the beads extremely carefully after the ethanol has dried. They blow out easily.

Index Kits

The index kits are expensive. Try to handle them with care and replace lids with the provided replacement lids, after every use. Multipipettes are absolutely essential to avoid loosing track on indexes used across the different wells on the plate. The use of the Illumina Plate Fixture comes in handy (see consumables below).

Amplicon QC

After the indexing-PCR products are cleaned it is recommended to run a gel on all samples to confirm that amplification was successful with enough DNA present and that it is the correct amplicon length. It also helps to identify any primer dimers, which may have to be cleaned out.

If you have access to a \textbf{Tapestation or a Bioanalyzer} in your lab, perhaps run a random subset of the indexed samples on one of these instruments, just to accurately confirm the average length of the amplicon. This average basepair length of the amplicon is important, as it is the basis for normalising your DNA to nM. In any case, after the index samples are normalised and pooled, it is highly recommended to run the pool on a Bioanalyser too. Otherwise it is too much guesswork in getting the concentrations right. This is further explained below.\\
Repeat any samples that failed, i.e.~that do not show any bands, and ensure you use the same indices.

\hfill\break

\hypertarget{normalisation-of-indexed-amplicons}{%
\subsection{Normalisation of indexed amplicons}\label{normalisation-of-indexed-amplicons}}

After you finished all PCR work it is time to do another round of Qubitting and normalisation of all samples into a new 96-well plate. The final pool should be 4 nM but we prefer to normalise DNA to a higher concentration first, say 10 nM, then pool and then dilute the pool to 4 nM afterwards.\\
We are using a very handy excel sheet, which was kindly developed and provided by \href{https://scholars.latrobe.edu.au/sknowler}{Sarah Knowler} at La Trobe University to prepare and assist in normalisations, converting from ng/µl to nM using the formula:

\[ concentrations~(ng/µl)  \over 660 g/mol~x~average~library~size\]
Contact us if you would like to get a copy of this very handy library preparation excel sheet.

Some more information here: \url{https://support.illumina.com/content/dam/illumina-support/documents/documentation/system_documentation/miseq/miseq-denature-dilute-libraries-guide-15039740-10.pdf}. There's also a pooling calculator here \url{https://support.illumina.com/help/pooling-calculator/pooling-calculator.htm} which may be helpful with your calculations.\\
\strut \\

\hypertarget{pooling}{%
\subsection{Pooling}\label{pooling}}

\begin{itemize}
\item
  Pipette equal volumes of the normalised DNA into one 1.5 ml tube.\\
\item
  Measure its concentration using the High Sensitivity (HS) Qubit 1X dsDNA HS Assay Kit.\\
\item
  Assuming the pool was created wtih normalised concentration that are higher than 4 nM, then you will need to dilute the pool down to 4nM. In order to do so you will need to accurately measure the average basepair length of your pool. Do not rely on the ladder on a gel. Always get an accurate read. See next point.\\
\item
  Measure the average base pair lengths of the pool with a \textbf{Bioanalyzer or Tapestation}. Use at least three replicates. The average basepair length is then used to calculate the final concentration required in ng/µl to get a 4 nM pool.\\
\item
  Dilute the pool to the concentration in ng/µl that reflect 4 nM.

  \hfill\break
\end{itemize}

\hypertarget{denaturing-of-pool-and-loading-of-the-miseq}{%
\subsection{Denaturing of pool and loading of the Miseq}\label{denaturing-of-pool-and-loading-of-the-miseq}}

Now you are almost ready load the Miseq. The last steps are done right before you plan to load the instrument. First the pool DNA needs to be denatured, mixed with PhiX and then diluted further to your preferred loading concentration (6 - 10 pM). The final concentrations impact on cluster densities. That means you get more data with higher concentrations but that is at the expense of potential errors during sequencing at high cluster densities. I am still not quite sure what the ideal concentration is. Please let me know if you know more.

The key for the final steps are that..

\begin{itemize}
\tightlist
\item
  ..the reagent cartridge is properly thawed. Perhaps thaw in fridge 3 hours before starting the denaturing steps. Then put into icy water to ensure final and even thawing across the cartridge. Mix cartrige well at the end and ensure that no bubbles are visible before loading.\\
\item
  .. the 0.2 N sodium hydroxide (NaOH) is made fresh with biological grade NaOH. For example use the \emph{BioUltra, for molecular biology, 10 M NaOH in H2O} from Sigma Aldrich to make a 0.2 N solution (N = Normality but it is equal to Molarity here because there is only one OH in NaOH). I have used analytical-grade NaOH pellets as well, which worked but the RNAse free solutions are safer in terms of inhibitors etc..\\
\item
  ..that the pool is cooled down immediately in ice or icy water right after the 2 minutes in the 96 degrees C heatblock and pretty much immediately loaded into the Miseq. The Miseq has to be prepped beforehand (I.e. maintenance wash and sample sheet loaded).\\
\item
  a sample sheet (or so-called manifest file) is created beforehand using the Illumina Experiment Manager Software. This software assist in producing a \texttt{.csv} file that is then loaded into the Miseq instrument via a USB and ensures that the indexes you have used are linked to the relevant samples in your pool. Basically a list of sample IDs and index IDs. An example of a sample sheet from one of our runs is shown below.\\
  \textbf{IMPORTANT}: Make sure to specify the Workflow as \textbf{GenerateFastQ}.\\
\item
  you have a Illumina BaseSpace account.
\end{itemize}

\hfill\break

\begin{figure}
\centering
\includegraphics{./img/manifestfile.png}
\caption{``Example of a manifest file''}
\end{figure}

\hypertarget{consumables}{%
\subsection{Consumables}\label{consumables}}

All consumables and equipment required for the 16S Miseq library prep protocol are available from page 21 of the above mentioned \href{https://sapac.support.illumina.com/content/dam/illumina-support/documents/documentation/chemistry_documentation/16s/16s-metagenomic-library-prep-guide-15044223-b.pdf}{16S Metagenomic Sequencing Library
Preparation} Guide.\\
The \textbf{magnetic stand} for 96-well plates and the TruSeq Index \textbf{Plate Fixture Kit} are essential.

Here is what we typically include into our purchasing list. The combined total of the below list costs over AUD\$ 10,000.

\begin{table}

\caption{\label{tab:consumables}Example of a consumables list we typically include for a Miseq run}
\centering
\begin{tabular}[t]{llll}
\toprule
Equipment.consumables & Item.number & Supplier & Comment\\
\midrule
Qiagen Powersoil Pro test kit, 250 & \#47016 & Qiagen & \\
Primers & Various & E.g. Sigma & \\
Magnetic beads,  AMPure XP / JetSeq Clean & BIO-68031 & Meridian Life Sciences & For clean-up\\
KAPA HiFi Hotstart ready mix & KK2602, 6.25ml & Millennium Science & For PCR\\
MiSeq Reagent Kit v3 & MS-102-3003 & Illumina & Needs to be as fresh as possible\\
\addlinespace
Nextera XT Index Kit v2, Set A, B, C or D & FC-131-2003 & Illumina & One set (A, B, C or D) for 96 uniquely  indexed samples\\
PhiX control & FC-110-3001 & Illumina & \\
Qubit 1X dsDNA BR Assay Kit & Q33266 & Thermofisher & DNA and PCR-product quantiication\\
Qubit 1X dsDNA HS Assay Kit & Q33230 & Thermofisher & Pool quantification\\
Bioanalyzer Kit (HighSens or 1000 Kit) & 5067-4626 or 5067-1504 & Integrated Science Tech & \\
\addlinespace
Tween 20 & H5151 & Promega & For Miseq instrument wash\\
96-well, 0.2ml plates, non-skirted & AB0600 & Thermofisher & For PCR\\
96 Well 0.8mL Polypropylene Deepwel Plate & AB0859 & Thermofisher & For clean-up\\
Optical Plate Seals & V7840 & Promega & For PCR\\
0.5mL thin-walled PCR tubes & E4942 & Promega & For Qubit\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{from-basespace-to-qiime2-and-dada2}{%
\chapter{From BaseSpace to Qiime2 and DADA2}\label{from-basespace-to-qiime2-and-dada2}}

\hypertarget{introduction-1}{%
\section{Introduction}\label{introduction-1}}

In this chapter you will learn how to process the FASTQ files that are generated from an amplicon sequencing run on a Miseq instrument. We are using Qiime2. Please also refer to the extensive documentation and tutorials available at \url{https://qiime2.org}. You can learn almost everything there.

\hypertarget{prerequisites-and-required-files}{%
\subsection{Prerequisites and required files}\label{prerequisites-and-required-files}}

\begin{itemize}
\tightlist
\item
  FASTQ files available to download on BaseSpace. Two for each sample. One for read 1 (R1) and another for read 2 (R2). For example, \texttt{PT-01\_S1\_L001\_R1\_001.fastq.gz} and \texttt{PT-01\_S1\_L001\_R2\_001.fastq.gz} contains all sequences for sample PT-01.
\item
  Personal computer or cloud computational resources with \textgreater{} 4 CPUS, \textgreater{} 16 RAM, \textgreater{} 100 GB storage recommended.\\
\item
  Cloud storage browser installed (example Cyberduck or PuTTYm see Chapter \ref{gettingstarted}).\\
\item
  Qiime2 installed. Always use the latest version.\\
\item
  All required files in correct format and file extension (see below).\\
\item
  The BaseSpace Downloader software (available on the BaseSpace page. You will be prompted to download once you go to your files).
\end{itemize}

\texttt{Qiimeimportmanifest.tsv} - A tab separated file with three columns where the first column is the sample ID (The exact same ID that was used in the manifest file for loading the Miseq). The second and third columns are the absolute paths to the forward and reverse FASTQ files, respectively. If you are working on a cloud computer then the FASTQ files should be uploaded to that, and the path needs to contain the full path to the relevant folder, including the full name of the file itself. If you process those files on your local drive then the paths need to change to your local folder containing the FASTQ files. In the below example table, the files are located on a Linux cloud computer (Nectar Research Cloud).

It might take some time to compile the import manifest. Each file path has to exact. However, in case you made a mistake in the file names etc, Qiime is generally pretty good at reporting where the error occured.

\begin{table}

\caption{\label{tab:qiimeimport}Example of a qiime import manifest.tsv file}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{l|l|l}
\hline
sample-id & forward-absolute-filepath & reverse-absolute-filepath\\
\hline
PT-01 & /pvol/Sequences/20220622\_PrimerTrial\_RMIT/PT-01\_S1\_L001\_R1\_001.fastq.gz & /pvol/Sequences/20220622\_PrimerTrial\_RMIT/PT-01\_S1\_L001\_R2\_001.fastq.gz\\
\hline
PT-02 & /pvol/Sequences/20220622\_PrimerTrial\_RMIT/PT-02\_S2\_L001\_R1\_001.fastq.gz & /pvol/Sequences/20220622\_PrimerTrial\_RMIT/PT-02\_S2\_L001\_R2\_001.fastq.gz\\
\hline
PT-03 & /pvol/Sequences/20220622\_PrimerTrial\_RMIT/PT-03\_S3\_L001\_R1\_001.fastq.gz & /pvol/Sequences/20220622\_PrimerTrial\_RMIT/PT-03\_S3\_L001\_R2\_001.fastq.gz\\
\hline
PT-04 & /pvol/Sequences/20220622\_PrimerTrial\_RMIT/PT-04\_S4\_L001\_R1\_001.fastq.gz & /pvol/Sequences/20220622\_PrimerTrial\_RMIT/PT-04\_S4\_L001\_R2\_001.fastq.gz\\
\hline
PT-05 & /pvol/Sequences/20220622\_PrimerTrial\_RMIT/PT-05\_S5\_L001\_R1\_001.fastq.gz & /pvol/Sequences/20220622\_PrimerTrial\_RMIT/PT-05\_S5\_L001\_R2\_001.fastq.gz\\
\hline
PT-06 & /pvol/Sequences/20220622\_PrimerTrial\_RMIT/PT-06\_S6\_L001\_R1\_001.fastq.gz & /pvol/Sequences/20220622\_PrimerTrial\_RMIT/PT-06\_S6\_L001\_R2\_001.fastq.gz\\
\hline
\end{tabular}
\end{table}

\hfill\break

\texttt{samplesheet.tsv} - file containing sample metadata. This file should contain all other environmental measurements you may have done on each sample (pH, EC\ldots), which you want to relate to microbial taxonomy and composition. The same file and format will be used to import metadata into the R package Phyloseq later too.

\begin{table}

\caption{\label{tab:metadata}Example of a samplesheet.tsv file}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{l|l|l|l|l|l|l}
\hline
\#SampleID & \#Sludge & WWTP & SludgeType & PMA\_treat & Rep\_bio & Sludgecollection\\
\hline
\#q2:types & categorical & categorical & categorical & categorical & numeric & categorical\\
\hline
PT-01 & 1 & ETP5 & Meso & NoPMA & 1 & 9/5/2022\\
\hline
PT-02 & 1 & ETP5 & Meso & NoPMA & 2 & 9/5/2022\\
\hline
PT-03 & 1 & ETP5 & Meso & NoPMA & 3 & 9/5/2022\\
\hline
PT-04 & 1 & ETP5 & Meso & NoPMA & 4 & 9/5/2022\\
\hline
PT-05 & 1 & ETP5 & Meso & NoPMA & 5 & 9/5/2022\\
\hline
\end{tabular}
\end{table}

\hfill\break

IMPORTANT: Create the \texttt{.tsv} file as a tab separated \texttt{.csv} file in Excel first and then manually change the file name from \texttt{.csv} to \texttt{.tsv}. Not sure how else to easily build the \texttt{.tsv} file.

\hfill\break

\texttt{scriptfile.txt} - A text file that contains your qiime2 scripts. Example below. Code is compiled here and then only the file itself is executed to run any commands in that text. The hash (\#) in front of any line of code stops it from being executed. That means all code is normally `hashed'. If one wants to run the code the lines are `un-hashed' and the text file is executed in the command line to run the `un-hashed' code. On the Linux cloud computer the command would be: \texttt{sh\ scriptfile.txt}.

Note that a script file is meant to make life easy and the process replicable. But you could run each of the processes directly by adding the commands into the terminal without a script file.

\begin{figure}
\centering
\includegraphics{./img/scriptfile.png}
\caption{``Example of a scriptfile.txt''}
\end{figure}

\hfill\break

Briefly on script files for cloud-based high performance computers (HPC) that are using Slurm to manage multiple user runs. If the HPC at your institution is using Slurm, then the script file has to be called with \texttt{sbatch\ scriptfile.txt}. The script files for HPCs using slurm contains a header that determins the allocations (number of CPUs, RAM etc..) for your task. The first lines of the .txt file typically look similar to this:

\begin{verbatim}
#!/bin/bash
#SBATCH --cpus-per-task=6
#SBATCH --mem-per-cpu=6000
#SBATCH --time=24:00:00
#SBATCH --partition=compute
#SBATCH --job-name=Q_fun
\end{verbatim}

Commands, such as this:

\begin{verbatim}
echo "Starting at: $(date)"
echo "Finished at: $(date)"
\end{verbatim}

.. are less important but help to report the start and end times of the executed commands from beginning (where the \texttt{echo} command is placed at the beginning of the script) to end (\texttt{echo}\ldots{} placed at the end of the script) into the Terminal Windows. They dont affect the qiime commands in any way.

Slurm and \texttt{sbatch} are not needed if you manage your own computer or your `own' cloud computer. In the example below we are using a Linux cloud computer, provided by the Nectar Research Cloud so we dont need Slurm.

\hypertarget{workflow-1}{%
\section{Workflow}\label{workflow-1}}

\hypertarget{download-fastq-files}{%
\subsection{Download FASTQ files}\label{download-fastq-files}}

In this example, Fastq files were produced from sequencing 16SrRNA marker genes on an Illumina Miseq instrument. If you ran your own library on your own BaseSpace account, the files should be available to download from that BaseSpace account. If the run was created by another user, that person can invite you to the run, which gives you access to download the files too.

Log in to BaseSpace and into your project. Go to files, run and download FastQ. You will be asked to install a downloader software. Follow instructions accordingly.\\
\includegraphics{./img/basespacedownload.png}

\includegraphics{./img/basespacedownload2.png}\\
\strut \\

The FASTQ files will come in separate folders for each sample. It is necessary to move all files into just ONE folder. That folder will then be used to import the sequences into a qiime object (described below).

\hypertarget{paired-end-manifest-import-step-1}{%
\subsection{Paired end manifest import (Step 1)}\label{paired-end-manifest-import-step-1}}

Go to the folder that contains all the FASTQ files (all files in one folder). I.e. in Unix/Linux use \texttt{cd\ path/to/folder/} to navigate to it. The scriptfile.txt should also be located here. The following qiime command that will look for the files and then create a qiime object using the sequences found in each FASTQ file. The \texttt{-\/-output-path} instructs qiime where to place and how to name the output file, here demux-paired-end.qza is placed into the same directory.\\
Remove any \# in front of your code (already removed in the below example) and execute the scriptfile.txt with \texttt{sh\ scriptfile.txt} (or \texttt{sbatch\ scriptfile.txt} in case you are using Slurm).

\begin{verbatim}
qiime tools import --type 'SampleData[PairedEndSequencesWithQuality]' \
  --input-path Qiimeimportmanifest.tsv \
  --output-path demux-paired-end.qza \
  --input-format PairedEndFastqManifestPhred33V2
\end{verbatim}

\hfill\break

\hypertarget{cutadapt-step-2}{%
\subsection{Cutadapt (Step 2)}\label{cutadapt-step-2}}

If all goes well you should now have a demux-paired-end.qza file in the same directory as your FASTQ files , which contains all your sequences. You could move demux-paired-end.qza to another folder and continue all other steps there if you wish. The FASTQ files are not needed anymore. Dont forget to move your script file to where it is most convenient for executing the commands. If you moved the demux-paired-end.qza to another folder it is probably easiest to also move the script file that the same directory (or create a second script file containing the remaining commands).

Go to the folder containing demux-paired-end.qza and use cutadapt to trim out the primer sequences. Unhash relevant lines in your script file, which should be located in the same folder. Run \texttt{sh\ scriptfile.txt}.

In this example we used the V4 primers. Change primer sequence to the exact primers that you used in the amplicon PCR.

\begin{verbatim}
qiime cutadapt trim-paired \
  --i-demultiplexed-sequences demux-paired-end.qza \
  --p-front-f GTGYCAGCMGCCGCGGTAA \
  --p-front-r GGACTACNVGGGTWTCTAAT \
  --o-trimmed-sequences trimmed_demux-paired-end.qza
\end{verbatim}

\hfill\break

\hypertarget{read-quality-assessment}{%
\subsection{Read quality assessment}\label{read-quality-assessment}}

\textbf{Visualise the output with a .qzv file}

The .qza files can be visualised by `converting' them into .qzv files.\\
Here we take the trimmed\_demux-paired-end.qza and create a trimmed\_demux-paired-end.qzv.\\
\textbf{NOTE:} View any .qzv file on \url{https://view.qiime2.org}. Drag and drop the qzv file into the browser window and inspect the results.

\begin{verbatim}
qiime demux summarize \
  --i-data trimmed_demux-paired-end.qza \
  --o-visualization trimmed_demux-paired-end.qzv
\end{verbatim}

Go to \url{https://view.qiime2.org} and drag and drop to visualise your .qzv file in the browser.

\includegraphics{./img/viewqiime.png}\\

Now click on the `Interactive Quality Plot' tab. You will see exactly that: Interactive Quality plots that look like this:\\
\includegraphics{./img/demux.qza1.png}\\
Note the total number of sequences. You want to retain a high percentage of these after quality filtering with DADA2.

\begin{figure}
\centering
\includegraphics{./img/demux.qza.png}
\caption{``Example of a demux.qzv visualisation''}
\end{figure}

\textbf{STOP HERE}. Inspect the visualisation and decide on location and maximum expected error.\\
From the output decide where to truncate the forward and reverse reads with \texttt{p-trunc-len-f}, \texttt{p-trunc-len-r},\texttt{-\/-p-max-ee-f} and \texttt{-\/-p-max-ee-r} in DADA2 below.

It can take some trial and error to get these setting right. We are using a docker-based package called FIGARO to help us estimate those parameters (Not shown here). But essentially you want to capture high quality reads and be confident about the Amplicon Sequence Variants (ASV), while also capture sufficent depth of ASVs and reads without unnecessarily filtering out too much. In the below example, we have trimmed the forward reads at 272 base pairs with a maximum expected error (max-ee) of 2 (which is the default) and the reverse reads at 151 with a max-ee of 3. I think it is o.k. to relax the max-ee for the reverse reads (which are ALWAYS lower in quality) as I feel more confident about the fact that the reverse reads are paired with the forward reads. Pairing in itself provides increased confidence that the reads do in fact represent a biological relevant sequence. As always please comment on our \href{https://github.com/chrismitbiz/ABlab-workflows/discussions/}{GitHub discussion page} if you have any suggestions here. Thanks!

A minimum overlap between the forward and reverse primer of 20 base pairs is recommended. To overlap can be calculated as following:

length forward read + length reverse read - length amplicon - truncated basepairs forward read - truncated basepairs reverse reads = overlap

So, for example, if we picked \texttt{-\/-p-trunc-len-f\ 272} and \texttt{-\/-p-trunc-len-r\ 151}, we get
\[301 + 301 - 292 - 29  - 150 = 131~bp~overlap\]

In cases where the quality of the reverse reads is very poor, or the amplicon is too long for pairing to work, it is also acceptable to import, trim and denoise only the forward reads. The V4 primer of this example, is nice and short at 292 basepairs, so is great for pairing even at lower reverse-read qualities.

\hypertarget{denoise-paired-end-sequences-with-dada2-step-3}{%
\subsection{Denoise paired end sequences with dada2 (Step 3)}\label{denoise-paired-end-sequences-with-dada2-step-3}}

Once the trimming and max-ee parameters are decided, it is time to run the DADA2 function. This may take a while, depending on total number of samples.

The output will be a feature\_table.qza and sample\_rep\_seqs.qza, containing the ASV abundances and their sequences respectively.

\begin{verbatim}
qiime dada2 denoise-paired \
  --i-demultiplexed-seqs trimmed_demux-paired-end.qza \
  --o-table feature_table.qza \
  --o-representative-sequences sample_rep_seqs.qza \
  --p-trim-left-f 0 --p-trim-left-r 0 \
  --p-trunc-len-f 271 \
  --p-trunc-len-r 151 \
    --p-max-ee-f 2 \
    --p-max-ee-r 3 \
  --output-dir dada2 \
  --verbose
\end{verbatim}

Summarise and visualise the ASV abundances (feature\_table.qza) in a .qzv file.

\begin{verbatim}
qiime feature-table summarize \
  --i-table feature_table.qza \
  --o-visualization feature_table.qzv \
  --m-sample-metadata-file samplesheet.tsv
\end{verbatim}

\textbf{Note:} Look at the feature\_table.qzv and record median reads per sample. Compare the total frequency after denoising with the total sequence count from the trimmed\_demux-paired-end.qza. You hope to retain a high percentage of total sequences after denoising with DADA2. It is prudent to report that percentage in your methods.

\begin{figure}
\centering
\includegraphics{./img/feature-table.qzv.png}
\caption{``feature\_table.qzv output''}
\end{figure}

\hfill\break

\hypertarget{taxonomic-classifier-and-assignment-step-4}{%
\subsection{Taxonomic classifier and assignment (Step 4)}\label{taxonomic-classifier-and-assignment-step-4}}

The next step is to assign taxonomies to the sequences in the denoised sample sequences. Here, we use a pre-trained classifier that is based on the Silva database. This pre-trained classifier is available on the data resource page of Qiime (Most current link at the time of writing: \url{https://docs.qiime2.org/2022.2/data-resources}).

However, in case you used a different primer you would have to create the classifier yourself. Again there is great resource avaiable on \url{https://docs.qiime2.org}. The process is fairly straight forward but takes computational time. Briefly, extract reference reads from a database (i.e.~Silva here) based on the primers used. Then use those extracted sequences and fit or train them onto a taxonomy. Basically, predict which amplicon sequence should be what phylym/class/order/family/genus etc\ldots. This trained file becomes a `classifier' that is used to assign taxonmies on your sequences.

Here we have a pre-trained classifier, silva-132-99-515-806-nb-classifier.qza:
The output is a file called taxonomy\_silva.qza and taxonomy\_silva.qzv in this case.

\begin{verbatim}
qiime feature-classifier classify-sklearn \
  --i-classifier silva-132-99-515-806-nb-classifier.qza \
  --p-reads-per-batch 10000 \
  --i-reads sample_rep_seqs.qza \
  --o-classification taxonomy_silva.qza \
  --quiet

# Then summarise and visualise the output into a .qza file

qiime metadata tabulate \
--m-input-file taxonomy_silva.qza \
--o-visualization taxonomy_silva.qzv
\end{verbatim}

\hfill\break

\hypertarget{build-phylogenetic-tree-step-5}{%
\subsection{Build phylogenetic tree (Step 5)}\label{build-phylogenetic-tree-step-5}}

The next step is not essential but really good to have. Creating a phylogenetic tree from the amplicon sequences.

In this case we are using the insertion tree method. See \url{https://library.qiime2.org/plugins/q2-fragment-insertion/16/} for more information in this method.

As not all ASVs will be inserted we will filter the feature\_table.qza again to keep only those ASVs that are in the tree. You will need the reference file from silva or greengenes. In this case we are using \texttt{sepp-refs-silva-128.qza}.

\begin{verbatim}
qiime fragment-insertion sepp \
  --i-representative-sequences sample_rep_seqs.qza \
  --i-reference-database sepp-refs-silva-128.qza \
  --o-tree insertion-tree.qza \
  --o-placements insertion-placements.qza

qiime fragment-insertion filter-features \
  --i-table feature_table.qza \
  --i-tree insertion-tree.qza \
  --o-filtered-table feature_table_insertiontreefiltered.qza \
  --o-removed-table removed_features.qza
\end{verbatim}

Done!

Everything else including further quality filtering happens with \texttt{phyloseq} in R where we will import the following files:
\texttt{feature\_table\_insertiontreefiltered.qza},
\texttt{taxonomy\_silva.qza} and
\texttt{insertion-tree.qza}.

This will be covered in the next chapter.

\hypertarget{all-steps-combined}{%
\subsection{All steps combined}\label{all-steps-combined}}

Copy and paste this into your script file if needed.

\begin{verbatim}
# Manifest Import
qiime tools import --type 'SampleData[PairedEndSequencesWithQuality]' \
  --input-path Qiimeimportmanifest.tsv \
  --output-path demux-paired-end.qza \
  --input-format PairedEndFastqManifestPhred33V2

# Cutadapt
qiime cutadapt trim-paired \
  --i-demultiplexed-sequences demux-paired-end.qza \
  --p-front-f GTGYCAGCMGCCGCGGTAA \
  --p-front-r GGACTACNVGGGTWTCTAAT \
  --o-trimmed-sequences trimmed_demux-paired-end.qza

qiime demux summarize \
  --i-data trimmed_demux-paired-end.qza \
  --o-visualization trimmed_demux-paired-end.qzv

# Denoise
qiime dada2 denoise-paired \
  --i-demultiplexed-seqs trimmed_demux-paired-end.qza \
  --o-table feature_table.qza \
  --o-representative-sequences sample_rep_seqs.qza \
  --p-trim-left-f 0 --p-trim-left-r 0 \
  --p-trunc-len-f 271 \
  --p-trunc-len-r 151 \
    --p-max-ee-f 2 \
    --p-max-ee-r 3 \
  --output-dir dada2 \
  --verbose
  
qiime feature-table summarize \
  --i-table feature_table.qza \
  --o-visualization feature_table.qzv \
  --m-sample-metadata-file samplesheet.tsv
  
# Taxonomic assignment
qiime feature-classifier classify-sklearn \
  --i-classifier silva-132-99-515-806-nb-classifier.qza \
  --p-reads-per-batch 10000 \
  --i-reads sample_rep_seqs.qza \
  --o-classification taxonomy_silva.qza \
  --quiet

# Phylogenetic tree
qiime fragment-insertion sepp \
  --i-representative-sequences sample_rep_seqs.qza \
  --i-reference-database sepp-refs-silva-128.qza \
  --o-tree insertion-tree.qza \
  --o-placements insertion-placements.qza

# Final filtering
qiime fragment-insertion filter-features \
  --i-table feature_table.qza \
  --i-tree insertion-tree.qza \
  --o-filtered-table feature_table_insertiontreefiltered.qza \
  --o-removed-table removed_features.qza
\end{verbatim}

Qiime2 reference:\\
Bolyen E, Rideout JR, Dillon MR, Bokulich NA, Abnet CC, Al-Ghalith GA, Alexander H, Alm EJ, Arumugam M, Asnicar F, Bai Y, Bisanz JE, Bittinger K, Brejnrod A, Brislawn CJ, Brown CT, Callahan BJ, Caraballo-Rodríguez AM, Chase J, Cope EK, Da Silva R, Diener C, Dorrestein PC, Douglas GM, Durall DM, Duvallet C, Edwardson CF, Ernst M, Estaki M, Fouquier J, Gauglitz JM, Gibbons SM, Gibson DL, Gonzalez A, Gorlick K, Guo J, Hillmann B, Holmes S, Holste H, Huttenhower C, Huttley GA, Janssen S, Jarmusch AK, Jiang L, Kaehler BD, Kang KB, Keefe CR, Keim P, Kelley ST, Knights D, Koester I, Kosciolek T, Kreps J, Langille MGI, Lee J, Ley R, Liu YX, Loftfield E, Lozupone C, Maher M, Marotz C, Martin BD, McDonald D, McIver LJ, Melnik AV, Metcalf JL, Morgan SC, Morton JT, Naimey AT, Navas-Molina JA, Nothias LF, Orchanian SB, Pearson T, Peoples SL, Petras D, Preuss ML, Pruesse E, Rasmussen LB, Rivers A, Robeson MS, Rosenthal P, Segata N, Shaffer M, Shiffer A, Sinha R, Song SJ, Spear JR, Swafford AD, Thompson LR, Torres PJ, Trinh P, Tripathi A, Turnbaugh PJ, Ul-Hasan S, van der Hooft JJJ, Vargas F, Vázquez-Baeza Y, Vogtmann E, von Hippel M, Walters W, Wan Y, Wang M, Warren J, Weber KC, Williamson CHD, Willis AD, Xu ZZ, Zaneveld JR, Zhang Y, Zhu Q, Knight R, and Caporaso JG. 2019. Reproducible, interactive, scalable and extensible microbiome data science using QIIME 2. Nature Biotechnology 37: 852--857. \url{https://doi.org/10.1038/s41587-019-0209-9}

\hypertarget{Qiime2R}{%
\chapter{From Qiime2 into R - Initial diagnostics}\label{Qiime2R}}

\hypertarget{introduction-2}{%
\section{Introduction}\label{introduction-2}}

In this chapter you will learn how to import Qiime2-produced ASV tables, taxonomy tables and tree files into R. For this exercise we will us publicly available FastQ files that were generated by a research group in Denmark. The files are available under accession number PRJNA645373.\\
Reference: {[}1{]} C. Jiang, S.J. McIlroy, R. Qi, F. Petriglieri, E. Yashiro, Z. Kondrotaite, P.H. Nielsen, Identification of microorganisms responsible for foam formation in mesophilic anaerobic digesters treating surplus activated sludge, Water Res. 191 (2021) 116779. \url{https://doi.org/10.1016/j.watres.2020.116779}.

Sample IDs\\
You decide on the samplesIDs before you start your library prep and sequencing. For this chapter we use a different dataset to the previous chatper and its sampleIDs are longer, e.g.~``SRR12204258'' and ``SRR12204269'', compared to ``PT-01'', ``PT-02'' in the previous chapter. It does not matter how you label each of your indexed samples as long as the IDs are unique. And each unique sample ID in the \texttt{samplesheet.csv} needs to match the sample IDs of the \texttt{feature\_table.qza}. You may recall those IDs originate from your Miseq run and are the same sample IDs that are used on the loading manifest file to prepare the run. They subsequently are part of the FastQ filenames and flow through into any relevant Qiime2 data. If a sample ID does not match or if there is a different number of samples between these two input files, then phyloseq will complain.\\
\strut \\

Here the first three Sample IDs (``SRR12204258'' and ``SRR12204269'' etc..) in the ASV table from this data set.:\\
\includegraphics{./img/ASVIDs.png}\\
\strut \\

ASV IDs\\
The same principle applies to each ID of the ASVs. The ASVs IDs have to match between the \texttt{feature\_table.qza} and the \texttt{taxonomy\_silva.qza}.

Following the first three ASV IDs (Feature ID) and related taxon assignments in taxonomy\_silva.qza of this dataset:\\
\includegraphics{./img/ASVIDs2.png}

\hfill\break

NOTE: After the denoising, taxonomic classification and tree alignments in Qiime2, I prefer to do all subsequent analysis in R. However, it is also possible to do much of the diversity analysis in Qiime2. In fact, some interesting plug-ins and functions in Qiime2 are not available as packages in R, hence for some specific tasks you may need to keep using Qiime2. It is up to the goals and preferences of the investigator.

Now let's work in R!

\textbf{Required files}

\begin{itemize}
\item
  \texttt{samplesheet.tsv} - same file that was used in the visualisation steps in Qiime2\\
\item
  \texttt{feature\_table.qza} - if you created a phylogenetic tree from the sequences as described in the previous chapter then the ASVs in the feature table were filtered to match the ASVs in the tree. Hence the files was named \texttt{featuretable-insertiontree-filtered.qza} to differentiate it.\\
\item
  \texttt{taxonomy\_silva.qza}\strut \\
\item
  \texttt{insertion-tree.qza}

  \hfill\break
\end{itemize}

\hypertarget{workflow-2}{%
\section{Workflow}\label{workflow-2}}

\hypertarget{packages}{%
\subsection{Packages}\label{packages}}

First install the required packages. Some packages are stored on `remote' repositories such as GitHub, hence before you can install them you need helper packages such as \texttt{remotes}. Other packages, such as \texttt{phyloseq} are managed by BioConductor project. They provide their own package manager package for R called \texttt{BiocManager}, which helps to install from the same release.

Learn more about phyloseq here: \url{https://joey711.github.io/phyloseq/}

Both, remotes and BiocManager are already installed in the below example.\\
Once the packages are installed, load them into your working space.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("remotes")}
\CommentTok{\# if (!requireNamespace("BiocManager", quietly = TRUE))}
\CommentTok{\#  install.packages("BiocManager")}
\CommentTok{\# remotes::install\_github("jbisanz/qiime2R")}
\CommentTok{\# BiocManager::install("phyloseq")}

\FunctionTok{library}\NormalTok{(qiime2R)  }\CommentTok{\# to import qiime.qza into an R object}
\FunctionTok{library}\NormalTok{(phyloseq) }\CommentTok{\# To combine all relevant data objects into one object for easy data management}
\FunctionTok{library}\NormalTok{(tidyverse) }\CommentTok{\# Compilation of packages for data management }
\FunctionTok{library}\NormalTok{(stringr)  }\CommentTok{\# to change some of the strings in taxonomic names in Silva }
\FunctionTok{library}\NormalTok{(vegan)   }\CommentTok{\# A commonly used package in numerical ecological }
\end{Highlighting}
\end{Shaded}

\hfill\break

\hypertarget{import-qiime-files-and-create-a-phyloseq-object}{%
\subsection{Import qiime-files and create a phyloseq object}\label{import-qiime-files-and-create-a-phyloseq-object}}

Now that packages are loaded we can import the Qiime2 \texttt{.qza} and the \texttt{.tsv} files. The aim is to import everything as R objects, which are then combined into one phyloseq object. Once we have a phyloseq object, any filtering and visualisations can be run from the one object.

In the previous chapter, we used a \texttt{samplesheet.tsv} to visualise and summarise the \texttt{feature\_table.qza}. That exact same samplesheet.tsv is imported here and slightly changed to make it phyloseq friendly. In this example, all relevant Qiime2 outputs were copied into a folder named `qiimefiles'.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sample sheet}
\NormalTok{metadata }\OtherTok{\textless{}{-}} \FunctionTok{read\_tsv}\NormalTok{(}\StringTok{"qiimefiles/samplesheet.tsv"}\NormalTok{)  }\CommentTok{\# lets call the R object \textquotesingle{}metadata\textquotesingle{} to reflect its purpose }
\CommentTok{\# Inspect the metadata object. The second row is qiime{-}specific information and has to be removed. }

\NormalTok{metadata2 }\OtherTok{\textless{}{-}}\NormalTok{ metadata[}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(metadata)),] }\SpecialCharTok{\%\textgreater{}\%}     \CommentTok{\# remove the top row and convert characters to factors}
  \FunctionTok{rownames\_to\_column}\NormalTok{(}\StringTok{"spl"}\NormalTok{)}\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# this just adds the rownames to a column and calles in "spl"}
  \FunctionTok{mutate\_all}\NormalTok{(type.convert) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate\_if}\NormalTok{(is.factor, as.character) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# reformatting columns to avoid any problems with factors at this stage}
  \FunctionTok{column\_to\_rownames}\NormalTok{(}\StringTok{"spl"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# this puts the column "spl" back into rownames}
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{column\_to\_rownames}\NormalTok{(}\StringTok{"\#SampleID"}\NormalTok{)}

\CommentTok{\# str(metadata2)  \# use the \textasciigrave{}str\textasciigrave{} command to inspect the data}

\CommentTok{\# Then decide which of the columns you want to be factors and in which order these factors should go. This will determine the order in which ggplot will plot them. You can include the levels = c("level1", "level"...)  command to determine the order, which is not done here. }
\NormalTok{metadata2  }\OtherTok{\textless{}{-}}\NormalTok{ metadata2 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Reactor =} \FunctionTok{factor}\NormalTok{(Reactor)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Location =} \FunctionTok{factor}\NormalTok{(loc))}

\CommentTok{\# Qiime import                      }
\NormalTok{SVs }\OtherTok{\textless{}{-}}\NormalTok{  qiime2R}\SpecialCharTok{::}\FunctionTok{read\_qza}\NormalTok{(}\StringTok{"qiimefiles/featuretable{-}insertiontree{-}filtered.qza"}\NormalTok{)}
\NormalTok{ASVtable }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(SVs}\SpecialCharTok{$}\NormalTok{data)   }\CommentTok{\# extract the data table, this has to be a dataframe}

\CommentTok{\# Taxonomy}
\NormalTok{taxonomy }\OtherTok{\textless{}{-}}\NormalTok{ qiime2R}\SpecialCharTok{::}\FunctionTok{read\_qza}\NormalTok{(}\StringTok{"qiimefiles/taxonomy\_silva.qza"}\NormalTok{) }\CommentTok{\# import the qiime object}
\NormalTok{taxonomy }\OtherTok{\textless{}{-}}\NormalTok{ taxonomy}\SpecialCharTok{$}\NormalTok{data  }\CommentTok{\# extract the taxonomy data}
\DocumentationTok{\#\# re{-}format the taxonomy file to split the taxonomy into columns}
\DocumentationTok{\#\# remove the confidence column}
\NormalTok{taxtable }\OtherTok{\textless{}{-}}\NormalTok{ taxonomy }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{separate}\NormalTok{(Taxon, }\AttributeTok{sep=}\StringTok{";"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"Kingdom"}\NormalTok{,}\StringTok{"Phylum"}\NormalTok{,}\StringTok{"Class"}\NormalTok{,}\StringTok{"Order"}\NormalTok{,}\StringTok{"Family"}\NormalTok{,}\StringTok{"Genus"}\NormalTok{,}\StringTok{"Species"}\NormalTok{))  }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Confidence) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{column\_to\_rownames}\NormalTok{(}\StringTok{"Feature.ID"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as.matrix}\NormalTok{()}

\CommentTok{\# Tree}
\NormalTok{tree }\OtherTok{\textless{}{-}} \FunctionTok{read\_qza}\NormalTok{(}\StringTok{"qiimefiles/insertion{-}tree.qza"}\NormalTok{)}
\NormalTok{tree }\OtherTok{\textless{}{-}}\NormalTok{ tree}\SpecialCharTok{$}\NormalTok{data  }\CommentTok{\#extract data}

\CommentTok{\# Create the phyloseq object }
\NormalTok{ps }\OtherTok{\textless{}{-}}\FunctionTok{phyloseq}\NormalTok{(}
  \FunctionTok{otu\_table}\NormalTok{(ASVtable, }\AttributeTok{taxa\_are\_rows =}\NormalTok{ T), }
  \FunctionTok{sample\_data}\NormalTok{(metadata2),}
\NormalTok{  phyloseq}\SpecialCharTok{::}\FunctionTok{tax\_table}\NormalTok{(taxtable),}
\NormalTok{  phyloseq}\SpecialCharTok{::}\FunctionTok{phy\_tree}\NormalTok{(tree)}
\NormalTok{)}

\CommentTok{\# ps  (not run)}
\CommentTok{\# phyloseq{-}class experiment{-}level object}
\CommentTok{\# otu\_table()   OTU Table:         [ 4218 taxa and 51 samples ]}
\CommentTok{\# sample\_data() Sample Data:       [ 51 samples by 14 sample variables ]}
\CommentTok{\# tax\_table()   Taxonomy Table:    [ 4218 taxa by 7 taxonomic ranks ]}
\CommentTok{\# phy\_tree()    Phylogenetic Tree: [ 4218 tips and 4217 internal nodes ]}

\CommentTok{\# remove things out of the R environment you dont need. }
\FunctionTok{rm}\NormalTok{(metadata, metadata2, SVs, taxonomy, taxtable, tree, ASVtable)}

\DocumentationTok{\#\# Remove those annoying short codes in front of taxa names (i.e. p\_\_ etc) as they}
\DocumentationTok{\#\# dont look good in visualisation}
\FunctionTok{tax\_table}\NormalTok{(ps)[, }\StringTok{"Kingdom"}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{str\_replace\_all}\NormalTok{(}\FunctionTok{tax\_table}\NormalTok{(ps)[, }\StringTok{"Kingdom"}\NormalTok{], }\StringTok{"d\_\_"}\NormalTok{, }\StringTok{""}\NormalTok{)}
\FunctionTok{tax\_table}\NormalTok{(ps)[, }\StringTok{"Phylum"}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{str\_replace\_all}\NormalTok{(}\FunctionTok{tax\_table}\NormalTok{(ps)[, }\StringTok{"Phylum"}\NormalTok{], }\StringTok{" p\_\_"}\NormalTok{, }\StringTok{""}\NormalTok{)}
\FunctionTok{tax\_table}\NormalTok{(ps)[, }\StringTok{"Class"}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{str\_replace\_all}\NormalTok{(}\FunctionTok{tax\_table}\NormalTok{(ps)[, }\StringTok{"Class"}\NormalTok{], }\StringTok{" c\_\_"}\NormalTok{, }\StringTok{""}\NormalTok{)}
\FunctionTok{tax\_table}\NormalTok{(ps)[, }\StringTok{"Order"}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{str\_replace\_all}\NormalTok{(}\FunctionTok{tax\_table}\NormalTok{(ps)[, }\StringTok{"Order"}\NormalTok{], }\StringTok{" o\_\_"}\NormalTok{, }\StringTok{""}\NormalTok{)}
\FunctionTok{tax\_table}\NormalTok{(ps)[, }\StringTok{"Family"}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{str\_replace\_all}\NormalTok{(}\FunctionTok{tax\_table}\NormalTok{(ps)[, }\StringTok{"Family"}\NormalTok{], }\StringTok{" f\_\_"}\NormalTok{, }\StringTok{""}\NormalTok{)}
\FunctionTok{tax\_table}\NormalTok{(ps)[, }\StringTok{"Genus"}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{str\_replace\_all}\NormalTok{(}\FunctionTok{tax\_table}\NormalTok{(ps)[, }\StringTok{"Genus"}\NormalTok{], }\StringTok{" g\_\_"}\NormalTok{, }\StringTok{""}\NormalTok{)}
\FunctionTok{tax\_table}\NormalTok{(ps)[, }\StringTok{"Species"}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{str\_replace\_all}\NormalTok{(}\FunctionTok{tax\_table}\NormalTok{(ps)[, }\StringTok{"Species"}\NormalTok{], }\StringTok{" s\_\_"}\NormalTok{, }\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

From the output of the \texttt{ps} object we can see that there are 4281 ASVs in 51 samples and associated with 14 variables in the metadata.

If you wish, you can access each individual data set from the ps object with functions \texttt{otu\_table(ps)@.Data}, \texttt{sample\_data(ps)} \texttt{tax\_table(ps)@.Data} or \texttt{phy\_tree(ps)}. This can become handy if you want to change something, such as adding columns to the sample\_data or changing them to a factor etc..

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ASVs }\OtherTok{\textless{}{-}} \FunctionTok{otu\_table}\NormalTok{(ps)}\SpecialCharTok{@}\NormalTok{.Data}
\NormalTok{metadata }\OtherTok{\textless{}{-}} \FunctionTok{sample\_data}\NormalTok{(ps)}
\NormalTok{taxtable }\OtherTok{\textless{}{-}} \FunctionTok{tax\_table}\NormalTok{(ps)}\SpecialCharTok{@}\NormalTok{.Data}
\NormalTok{tree }\OtherTok{\textless{}{-}} \FunctionTok{phy\_tree}\NormalTok{(ps)}

\CommentTok{\# Inspect individual objects in your own time. }
\end{Highlighting}
\end{Shaded}

\hfill\break

\hypertarget{save-the-ps-object-as-an-.rds-file-into-your-working-directory}{%
\subsection{\texorpdfstring{Save the \texttt{ps} object as an \texttt{.rds} file into your working directory}{Save the ps object as an .rds file into your working directory}}\label{save-the-ps-object-as-an-.rds-file-into-your-working-directory}}

Optional\\
This step allows you to save and store the R object as a RDS file and load it back into any future R environment, should you require it. This would be helpful if your workflow is separated into different Rscripts and you want to just load in the RDS file instead of having to import qiime files and create the phyloseq object every time.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{saveRDS}\NormalTok{(ps, }\AttributeTok{file =} \StringTok{"ps\_ProjectX\_2022July"}\NormalTok{)}

\CommentTok{\# to load the file back into your R environment:}
\CommentTok{\# ps \textless{}{-} readRDS(file = "ps\_ProjectX\_2022July")}
\end{Highlighting}
\end{Shaded}

\hypertarget{initial-filtering}{%
\subsection{Initial filtering}\label{initial-filtering}}

Keep the orginal \texttt{ps} object unchanged. Create new objects for subsequent filtering steps.

Often we simply filter out any ASVs that have less than x number of reads and are present in less than x number of samples. This helps to reduce noise and the overload of zeros in the dataset, which some differential abundance tests can't deal with. Perhaps, it also helps with removing ASVs that were incorrectly denoised with DADA2 in the first place. However, it also means that rare ASVs with a low prevalence are not considered. So filtering has to be done with consideration to the intended analysis.

Then filter out any phyla that came up as uncharacterized and taxa that came up as mitochondria and chloroplast.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# filter as needed. This will be your final otu{-}table.}
\CommentTok{\# Although you can still filter for specific analysis if needed.}
\CommentTok{\# minimum of reads per feature}
\NormalTok{ps.flt }\OtherTok{=} \FunctionTok{prune\_taxa}\NormalTok{(}\FunctionTok{taxa\_sums}\NormalTok{(ps) }\SpecialCharTok{\textgreater{}=} \DecValTok{5}\NormalTok{, ps) }\CommentTok{\#minimum reads per feature}

\CommentTok{\# ps.flt (not run)}
\CommentTok{\# phyloseq{-}class experiment{-}level object}
\CommentTok{\# otu\_table()   OTU Table:         [ 3911 taxa and 51 samples ]}
\CommentTok{\# sample\_data() Sample Data:       [ 51 samples by 14 sample variables ]}
\CommentTok{\# tax\_table()   Taxonomy Table:    [ 3911 taxa by 7 taxonomic ranks ]}
\CommentTok{\# phy\_tree()    Phylogenetic Tree: [ 3911 tips and 1694 internal nodes ]}

\CommentTok{\#filter any "NA"{-}phyla that have not been classified i.e. }
\CommentTok{\# contain nothing in the phylum column of the taxtable (just a \textless{}NA\textgreater{})}

\NormalTok{ps.flt  }\OtherTok{=} \FunctionTok{subset\_taxa}\NormalTok{(ps.flt , }\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(Phylum) }\SpecialCharTok{\&} \SpecialCharTok{!}\NormalTok{Phylum }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{""}\NormalTok{))}

\CommentTok{\# ps.flt (not run)}
\CommentTok{\# phyloseq{-}class experiment{-}level object}
\CommentTok{\# otu\_table()   OTU Table:         [ 3892 taxa and 51 samples ]}
\CommentTok{\# sample\_data() Sample Data:       [ 51 samples by 14 sample variables ]}
\CommentTok{\# tax\_table()   Taxonomy Table:    [ 3892 taxa by 7 taxonomic ranks ]}
\CommentTok{\# phy\_tree()    Phylogenetic Tree: [ 3892 tips and 1687 internal nodes ]}
  
\CommentTok{\# Filter out non{-}bacteria, mitochondia and chloroplast taxa}
\NormalTok{ps.flt }\OtherTok{\textless{}{-}}\NormalTok{ ps.flt }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{subset\_taxa}\NormalTok{(Kingdom }\SpecialCharTok{==} \StringTok{"Bacteria"} \SpecialCharTok{\&}\NormalTok{ Family  }\SpecialCharTok{!=} \StringTok{"Mitochondria"} \SpecialCharTok{\&}\NormalTok{ Class   }\SpecialCharTok{!=} \StringTok{"Chloroplast"}\NormalTok{)}

\CommentTok{\# ps.flt (not run)}
\CommentTok{\# phyloseq{-}class experiment{-}level object}
\CommentTok{\# otu\_table()   OTU Table:         [ 3853 taxa and 51 samples ]}
\CommentTok{\# sample\_data() Sample Data:       [ 51 samples by 14 sample variables ]}
\CommentTok{\# tax\_table()   Taxonomy Table:    [ 3853 taxa by 7 taxonomic ranks ]}
\CommentTok{\# phy\_tree()    Phylogenetic Tree: [ 3853 tips and 1673 internal nodes ]}

\CommentTok{\# We can filter more for running beta diversity or differential abundance analysis later. This will be shown in later chapters}
\CommentTok{\# For example: }
\CommentTok{\# minimum presence in x\% of samples (51 * 0.04 = 2 samples)}
\CommentTok{\# ps.flt = filter\_taxa(ps.flt, function(x) sum(x \textgreater{} 0) \textgreater{} (0.04*length(x)), TRUE) }
\CommentTok{\# Would result in }
\CommentTok{\# phyloseq{-}class experiment{-}level object}
\CommentTok{\# otu\_table()   OTU Table:         [ 1674 taxa and 51 samples ]}
\CommentTok{\# sample\_data() Sample Data:       [ 51 samples by 14 sample variables ]}
\CommentTok{\# tax\_table()   Taxonomy Table:    [ 1674 taxa by 7 taxonomic ranks ]}
\CommentTok{\# phy\_tree()    Phylogenetic Tree: [ 1674 tips and 1673 internal nodes ]}
\end{Highlighting}
\end{Shaded}

\hfill\break

\hypertarget{rarefaction-curve}{%
\subsection{Rarefaction curve}\label{rarefaction-curve}}

Check if you cover enough depth for diversity analyses. Some samples may have low species richness, including a negative or samples that failed during PCR for example. The rarefaction curve will help assess which samples you may have to exclude from subsequent diversity calculations. It also provides a general view if samples cover enough of the potential richness (Number of ASVs).

The curve shows you the number of `Species' (i.e.~ASVs) on the y axis and the total reads on the x axis for each sample. So basically, it shows how rich each sample is and indicates if the number of reads that are captured in a samples covers enough of the ASVs. If the curve flattens, it indicates that the sample would not get any richer even with more reads\ldots{}

I typically use the rarefaction curve to check if there are samples that need removing before establishing alpha diversity metrics. In this example, the vertical line helps identify the sample with lowest number of reads. We can check that by running \texttt{min(colSums(otu\_table(ps.flt)))}, which tells us that they are 9690 reads in the sample with lowest reads. If this sample is kept in and we measure subsequent alpha diversity metrics, where each of the sample-reads are randomly resampled to the lowest read-number (here 9690), then we may not capture the diversity of samples with higher richness. The grey horizontal lines indicate how much richness we may lose if we include the sample with lowest number of reads (the difference between grey horizontal line and the curve of each sample at the highest number of reads on x-axis).

\hfill\break

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vegan}\SpecialCharTok{::}\FunctionTok{rarecurve}\NormalTok{(}\FunctionTok{t}\NormalTok{(}\FunctionTok{otu\_table}\NormalTok{(ps.flt)), }
                 \AttributeTok{step=}\DecValTok{200}\NormalTok{, }\AttributeTok{sample =} \FunctionTok{min}\NormalTok{(}\FunctionTok{colSums}\NormalTok{(}\FunctionTok{otu\_table}\NormalTok{(ps.flt))), }
                 \AttributeTok{label =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"Sample Size after filtering"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\includegraphics{gitbook-demo_files/figure-latex/rarefaction-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# show the lowest and highest number of sample reads.  }
\FunctionTok{max}\NormalTok{(}\FunctionTok{colSums}\NormalTok{(}\FunctionTok{otu\_table}\NormalTok{(ps.flt)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 42345
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{min}\NormalTok{(}\FunctionTok{colSums}\NormalTok{(}\FunctionTok{otu\_table}\NormalTok{(ps.flt)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 9770
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filtering options}
\CommentTok{\# filter options (not run), you can filter out specific sample or treatments from the phyloseq objects. }
\CommentTok{\# \textasciigrave{}\%notin\%\textasciigrave{} = Negate(\textasciigrave{}\%in\%\textasciigrave{})  \# create a \%notin\% filter operator for filtering}

\CommentTok{\# Filtering out treatments, e.g. any sample that was labeled "Negative" in the metadata}
\CommentTok{\# ps.flt.flt \textless{}{-}  prune\_samples(sample\_data(ps.flt)$Treatment \%notin\% c("Negative"), ps.flt)}

\CommentTok{\# Filtering out any taxa that have zero reads because they were only present in the removed samples.  }
\CommentTok{\# ps.flt.flt \textless{}{-} prune\_taxa(taxa\_sums(ps.flt.flt) != 0, ps.flt.flt)}
\end{Highlighting}
\end{Shaded}

Once you identified the samples you would like to remove, you can filter out specific samples or treatments from the phyloseq objects. For example, with the \texttt{prune\_samples(sample\_data(ps.flt)\$Treatment\ \%notin\%\ c("Negative"),\ ps.flt)} command here we would sample out any samples labeled as ``Negative'' in the metadata. Afterwards make sure to sample out any taxa that now contain zero reads because they were only present in the removed samples. Perhaps, create new phyloseq objects to differentiate from your different filtered object \texttt{ps.flt.flt\ \textless{}-\ prune\_taxa(taxa\_sums(ps.flt)\ !=\ 0,\ ps.flt)}.

In this example, we are keeping the sample with lowest reads. No further filtering required at this stage.

\hfill\break

\hypertarget{prevalence-table}{%
\subsection{Prevalence table}\label{prevalence-table}}

Create a prevalence table to get an overview of phyla with low to high prevalence.

The output is a dataframe showing all phyla in order of total abundance. The mean prevalence defines how often the phylum appears across all samples on average. E.g. a mean prevalence of 10.22 means that ASVs in this phylum were present on average in 10.22 samples. Higher values indicates a `core' relevance to the sum of samples.

This table is handy to decide if you want to remove certain phyla from some visualisations as they may not contribute to the overall analysis. It also allows for a comparison with filtered and unfiltered data for due diligence. Here it is apparent that phyla that were unclassified (i.e.~ ) contributed 3,522 reads to the total of 1,299,791 reads. That is \texttt{3522\ /\ sum(phyloseq::otu\_table(ps))\ *\ 100} = 0.27\%. Chances are that these NA sequences may just be noise and may not represent biological relevant amplicons. Hence, I would keep them out of any further analysis.\\
However, in environments where you get higher contributions of unknown amplicons, it may indicate that the taxonomic database does not capture your samples well. In that case it would be wise to keep the NAs in for measuring alpha and beta diversity.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Create a prevalence table to see which phyla have low prevalence}
\CommentTok{\#ps.flt.prev \textless{}{-}  prune\_samples(sample\_data(physeqB.flt)$Treatment != "Reference", physeqB.flt)}
\CommentTok{\# ps.flt.prevtab \textless{}{-}  prune\_samples(sample\_data(physeqB.flt.prevtab)$Treatment != "Blank", physeqB.flt.prevtab)}


\NormalTok{prevelancedf }\OtherTok{=} \FunctionTok{apply}\NormalTok{(}\AttributeTok{X =}\NormalTok{ phyloseq}\SpecialCharTok{::}\FunctionTok{otu\_table}\NormalTok{(ps.flt),}
                       \AttributeTok{MARGIN =} \DecValTok{1}\NormalTok{,}
                       \AttributeTok{FUN =} \ControlFlowTok{function}\NormalTok{(x)\{}\FunctionTok{sum}\NormalTok{(x }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{)\})}

\NormalTok{prevelancedf }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Prevalence =}\NormalTok{ prevelancedf,}
                            \AttributeTok{TotalAbundance =} \FunctionTok{taxa\_sums}\NormalTok{(ps.flt),}
\NormalTok{                            phyloseq}\SpecialCharTok{::}\FunctionTok{tax\_table}\NormalTok{(ps.flt))}

\NormalTok{prevelancedf }\OtherTok{\textless{}{-}}\NormalTok{ plyr}\SpecialCharTok{::}\FunctionTok{ddply}\NormalTok{(prevelancedf, }\StringTok{"Phylum"}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(df1)\{}
  \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{mean\_prevalence=}\FunctionTok{mean}\NormalTok{(df1}\SpecialCharTok{$}\NormalTok{Prevalence), }\AttributeTok{total\_abundance=}\FunctionTok{sum}\NormalTok{(df1}\SpecialCharTok{$}\NormalTok{TotalAbundance,}\AttributeTok{na.rm =}\NormalTok{ T), }\AttributeTok{stringsAsFactors =}\NormalTok{ F)}
\NormalTok{  \})}

\NormalTok{prevelancedf }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(total\_abundance))  }\CommentTok{\# print table from highest to lowest abundance }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                           Phylum mean_prevalence total_abundance
## 1                    Chloroflexi        5.124069          228197
## 2                   Bacteroidota        4.683206          200707
## 3                     Firmicutes        4.420402          188599
## 4               Actinobacteriota        5.304569          165409
## 5                 Proteobacteria        4.263459          161215
## 6               Desulfobacterota        5.470000           57102
## 7                   Synergistota        6.595506           51225
## 8                 Cloacimonadota        6.717391           45260
## 9                Acidobacteriota        3.976190           35144
## 10                  Thermotogota       10.190476           33489
## 11                 Spirochaetota        4.578431           25015
## 12             Verrucomicrobiota        3.430233           16537
## 13             Fermentibacterota       35.000000           14812
## 14               Patescibacteria        3.681818           14380
## 15            Caldatribacteriota        5.500000            8932
## 16               Planctomycetota        2.518519            5138
## 17                Armatimonadota        2.933333            4443
## 18 Marinimicrobia_(SAR406_clade)       20.000000            3818
## 19               Hydrogenedentes        3.950000            3411
## 20                  Nitrospirota        8.750000            2853
## 21              Campilobacterota        4.187500            2710
## 22  SAR324_clade(Marine_group_B)        4.000000            2616
## 23                           WS1        3.625000            2559
## 24                Fibrobacterota        2.818182            1870
## 25                   Myxococcota        3.176471            1353
## 26              Bdellovibrionota        3.600000            1111
## 27                 Caldisericota        3.818182            1067
## 28                         WPS-2        2.666667             897
## 29                   Sumerlaeota        3.888889             683
## 30                 Cyanobacteria        9.000000             647
## 31                Fusobacteriota        2.888889             565
## 32          Coprothermobacterota        2.800000             511
## 33               Gemmatimonadota        2.875000             300
## 34               Elusimicrobiota        2.833333             225
## 35                      CK-2C2-2        4.000000             162
## 36             Methylomirabilota        4.000000             159
## 37              Latescibacterota        2.333333              86
## 38                  Dependentiae        2.500000              78
## 39                  Zixibacteria        5.000000              74
## 40                       Sva0485        2.333333              66
## 41                  Acetothermia        2.000000              59
## 42                           WS4        1.500000              39
## 43                  Hydrothermae        1.000000              26
## 44                  Deinococcota        1.000000              11
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Compare with the original, unfiltered ps object}
\NormalTok{prevelancedf }\OtherTok{=} \FunctionTok{apply}\NormalTok{(}\AttributeTok{X =}\NormalTok{ phyloseq}\SpecialCharTok{::}\FunctionTok{otu\_table}\NormalTok{(ps),}
                       \AttributeTok{MARGIN =} \DecValTok{1}\NormalTok{,}
                       \AttributeTok{FUN =} \ControlFlowTok{function}\NormalTok{(x)\{}\FunctionTok{sum}\NormalTok{(x }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{)\})}

\NormalTok{prevelancedf }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Prevalence =}\NormalTok{ prevelancedf,}
                            \AttributeTok{TotalAbundance =} \FunctionTok{taxa\_sums}\NormalTok{(ps),}
\NormalTok{                            phyloseq}\SpecialCharTok{::}\FunctionTok{tax\_table}\NormalTok{(ps))}

\NormalTok{prevelancedf }\OtherTok{\textless{}{-}}\NormalTok{ plyr}\SpecialCharTok{::}\FunctionTok{ddply}\NormalTok{(prevelancedf, }\StringTok{"Phylum"}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(df1)\{}
  \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{mean\_prevalence=}\FunctionTok{mean}\NormalTok{(df1}\SpecialCharTok{$}\NormalTok{Prevalence), }\AttributeTok{total\_abundance=}\FunctionTok{sum}\NormalTok{(df1}\SpecialCharTok{$}\NormalTok{TotalAbundance,}\AttributeTok{na.rm =}\NormalTok{ T), }\AttributeTok{stringsAsFactors =}\NormalTok{ F)}
\NormalTok{  \})}

\NormalTok{prevelancedf }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(total\_abundance))  }\CommentTok{\# print table from highest to lowest abundance }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                           Phylum mean_prevalence total_abundance
## 1                    Chloroflexi        4.865429          228276
## 2                   Bacteroidota        4.451499          202022
## 3                     Firmicutes        4.283019          197782
## 4               Actinobacteriota        5.045131          165791
## 5                 Proteobacteria        4.046709          161897
## 6               Desulfobacterota        4.829060           57187
## 7                   Synergistota        6.533333           51227
## 8                 Cloacimonadota        6.387755           45296
## 9                Acidobacteriota        3.791209           35318
## 10                  Thermotogota       10.190476           33489
## 11                 Spirochaetota        4.443396           25028
## 12             Verrucomicrobiota        2.981308           16649
## 13             Fermentibacterota       35.000000           14812
## 14               Patescibacteria        3.138554           14501
## 15            Caldatribacteriota        5.304348            8934
## 16               Planctomycetota        2.433333            5478
## 17                Armatimonadota        2.611111            4462
## 18 Marinimicrobia_(SAR406_clade)       20.000000            3818
## 19                          <NA>        3.636364            3522
## 20               Hydrogenedentes        3.950000            3411
## 21                  Nitrospirota        8.750000            2853
## 22              Campilobacterota        4.187500            2710
## 23  SAR324_clade(Marine_group_B)        3.333333            2622
## 24                           WS1        3.625000            2559
## 25                Fibrobacterota        2.750000            1942
## 26                   Myxococcota        2.681818            1369
## 27              Bdellovibrionota        2.444444            1133
## 28                 Caldisericota        3.583333            1071
## 29                         WPS-2        2.666667             897
## 30                   Sumerlaeota        3.600000             686
## 31                 Cyanobacteria        9.000000             647
## 32                Fusobacteriota        2.888889             565
## 33          Coprothermobacterota        2.800000             511
## 34               Gemmatimonadota        2.666667             304
## 35               Elusimicrobiota        2.833333             225
## 36                      CK-2C2-2        4.000000             162
## 37             Methylomirabilota        4.000000             159
## 38                  Dependentiae        1.750000              94
## 39              Latescibacterota        2.000000              89
## 40                  Zixibacteria        5.000000              74
## 41                       Sva0485        2.000000              69
## 42                  Acetothermia        2.000000              59
## 43                           WS4        1.250000              46
## 44                  Hydrothermae        1.000000              26
## 45                  Deinococcota        1.000000              11
## 46                       FCPU426        1.000000               3
## 47              Margulisbacteria        1.000000               3
## 48                        MBNT15        1.000000               2
\end{verbatim}

\textbf{Note:}\\
This code is an amalgamation from various sources. Apart from putting it together into this workflow I do not take credit for it.

  \bibliography{book.bib,packages.bib}

\end{document}
